# SecTransCNN
### 摘要
蛋白质二级结构是连接氨基酸一级序列与三维空间结构的重要中间层级，既反映了蛋白质的局部构象特征，也为更高层次的结构预测提供有效约束。因此，二级结构预测长期以来一直是蛋白质结构预测中的核心问题之一。蛋白质二级结构的形成同时受到远端残基间的长程相互作用以及局部序列环境**（如邻近残基的理化性质与空间限制）的共同影响。基于这一特点，本文尝试在 Transformer 架构中引入卷积层：利用自注意力机制建模全局范围内的长距离依赖关系，同时通过卷积操作强化模型对局部序列模式的建模能力。实验结果表明，相较于仅使用 Transformer 的基线模型，引入卷积层后模型在二级结构预测任务上的准确率有明显提升。但整体性能仍存在较大提升空间。本文在讨论部分进一步分析了可能的限制因素，并提出了后续的改进方向。

### 背景
Transformer 架构最初被提出用于序列到序列（Seq2Seq）任务，其核心思想是通过自注意力机制显式建模序列中任意两个 Token 之间的依赖关系。这一设计弱化了序列位置带来的限制，有效缓解了传统 RNN 和 LSTM 在长序列建模中常见的长期依赖难以捕获以及梯度消失或爆炸等问题。卷积神经网络（CNN）则以其局部感受野和参数共享的特点，擅长从局部上下文中提取稳定的模式信息，广泛应用于计算机视觉和医学影像分析等任务。在序列建模场景中，卷积层同样能够有效捕捉相邻元素之间的局部关联。
蛋白质是由约 20 种标准氨基酸按特定顺序连接形成的线性高分子，其序列组合空间极其庞大。蛋白质二级结构通常依据 DSSP（Define Secondary Structure of Proteins）算法进行标注，该算法基于蛋白质三维结构坐标，通过几何与能量判据识别主链之间的氢键网络，从而推断局部构象特征。基于氢键的局部构型，DSSP 将残基划分为八类二级结构状态（DSSP8），并在实际建模任务中通常进一步归并为三类（DSSP3）：螺旋（H）、β 链（E）和环区（C）。蛋白质中氨基酸的线性排列顺序被称为一级结构（即一维序列表示）。
在蛋白质二级结构预测任务中，氨基酸序列既表现出明显的长距离依赖特性，又强烈依赖于局部残基所形成的微环境，例如局部极性分布、空间位阻以及二级结构片段的连续性等因素。单纯依赖 Transformer 的全局注意力机制，可能会削弱模型对这些局部结构信号的敏感性，从而限制预测性能。基于上述考虑，本项目尝试将 Transformer 与卷积层相结合：首先利用 Transformer 建模序列范围内的全局相互作用，再通过卷积操作对局部序列特征进行进一步建模，以期在全局信息与局部信息之间取得更好的平衡。

### 结论

